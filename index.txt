1:"$Sreact.fragment"
2:I[7558,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"ThemeProvider"]
3:I[9994,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
b:I[7150,[],""]
:HL["/_next/static/css/3c8b863845cad745.css","style"]
0:{"P":null,"b":"mP7IeFZoPsAMh7wVG5Zku","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/3c8b863845cad745.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Chenrui Tie (ÈìÅÂÆ∏Áùø)","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L6","$L7"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L8"]}]}]]}]]}],{"children":["__PAGE__","$L9",{},null,false]},null,false],"$La",false]],"m":"$undefined","G":["$b",[]],"s":false,"S":true}
c:I[7923,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"default"]
d:I[9178,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
e:I[470,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
f:I[5078,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
10:I[2597,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
14:I[4431,[],"ViewportBoundary"]
16:I[4431,[],"MetadataBoundary"]
17:"$Sreact.suspense"
6:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
7:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
8:["$","$Lc",null,{"lastUpdated":"November 24, 2025"}]
11:T595,Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human.12:T481,Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks. However, extensive demonstrations are required for policy robustness and generalization. To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level SE(3) equivariant diffusion model for generating action sequences in complex robot manipulation tasks. Further, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints. We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner. We evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object. Experiments demonstrate superior data efficiency and manipulation proficiency of our proposed method, as well as its ability to generalize to unseen configurations with only a few demonstrations.9:["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$Ld",null,{"author":{"name":"Chenrui Tie","title":"PhD Student","institution":"National University of Singapore","avatar":"/bio.jpeg"},"social":{"email":"chenrui.tie@u.nus.edu","location":"Singapore","location_url":"https://maps.google.com/?q=National+University+of+Singapore","location_details":["School of Computing,","National University of Singapore"],"google_scholar":"https://scholar.google.com/citations?user=lN8cZMMAAAAJ&","orcid":"","github":"https://github.com/crtie/","linkedin":""},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Embodied AI","Data-Efficient Robot Learning","Generalizable Manipulation Skill","Robotic Assembly"],"keywords":["Robot Manipulation","Non-Prehensile Manipulation","Foundation Models","Task and Motion Planning","Robot Manipulation","Robotic Assembly","Foundation Models","Robot Manipulation","Robotic Assembly","Foundation Models","Robot Manipulation","Imitation Learning","SE(3) Equivariance","Diffusion Policy","Robot Manipulation","Affordance Learning","SE(3) Equivariance","Robot Manipulation","Foundation Models","Robot Manipulation","Differentiable Physics","Robot Manipulation","Robotic Assembly","SE(3) Equivariance"],"maxKeywords":5}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$Le","about",{"content":"I am a second-year Ph.D. student in the School of Computing, National University of Singapore, supervised by Prof. [Lin Shao](https://linsats.github.io/).\nBefore this, I received my B.S. degree from School of EECS, Peking University in 2024. In my undergraduate study, I am privileged to work with Prof. [Hao Dong](https://zsdonghao.github.io/) and Prof. [He Wang](https://hughw19.github.io/).\n\nMy research focuses on enabling robots to **efficiently** acquire **generalizable** manipulation skills. Specifically, I aim to (1) improve data efficiency by incorporating physics priors such as SE(3) equivariance into learning frameworks, and (2) enhance generalization by leveraging high-level abstractions from vision-language models and hierarchical task representations. It's worth mentioning that before studying computer science, I majored in physics, which naturally inspires me to integrate physical intuitions into robotic learning systems.","title":"About"}],["$","$Lf","news",{"items":[{"date":"2025-04","content":"Our work Manual2Skill has been accepted by RSS 2025 üéâ"},{"date":"2025-01","content":"Our work ET-SEED has been accepted by ICLR 2025 üéâ"},{"date":"2024-08","content":"Starting my PhD at National University of Singapore, supervised by Prof. Lin Shao"}],"title":"News"}],["$","$L10","featured_publications",{"publications":[{"id":"tie2025manual2skill","title":"Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Shengxiang Sun","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Jinxuan Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiwei Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingxiang Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yue Hu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haonan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junting Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":["Robot Manipulation","Robotic Assembly","Foundation Models"],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"Robotics: Science and Systems (RSS)","url":"https://arxiv.org/abs/2502.10090","code":"https://github.com/owensun2004/Manual2Skill","website":"https://owensun2004.github.io/Furniture-Assembly-Web/","abstract":"$11","description":"We propose a novel framework that enables VLM to understand human-designed manuals and acquire robotic skills for furniture assembly tasks.","selected":true,"preview":"chair.gif","bibtex":"@inproceedings{tie2025manual2skill,\n  title={Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Foundation MÂì¶ÁöÑÁªøËâ≤},\n  author={Tie, Chenrui and Sun, Shengxiang and Zhu, Jinxuan and Liu, Yiwei and Guo, Jingxiang and Hu, Yue and Chen, Haonan and Chen, Junting and Wu, Ruihai and Shao, Lin},\n  booktitle={Robotics: Science and Systems (RSS)},\n  year={2025}\n}\n"},{"id":"tie2024et","title":"ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yue Chen","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Boxuan Dong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zeyi Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chongkai Gao‚Ä†","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Dong‚Ä†","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":["Robot Manipulation","Imitation Learning","SE(3) Equivariance","Diffusion Policy"],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","url":"https://arxiv.org/abs/2411.03990","code":"https://github.com/yuechen0614/ET-SEED","website":"https://et-seed.github.io/","abstract":"$12","description":"We propose a new diffusion policy method to tackle tasks with certain symmetry, achieving better data efficiency and spatial generalization.","selected":true,"preview":"etseed.png","bibtex":"@article{tie2024seed,\n  title={Et-seed: Efficient trajectory-level se (3) equivariant diffusion policy},\n  author={Tie, Chenrui and Chen, Yue and Wu, Ruihai and Dong, Boxuan and Li, Zeyi and Gao, Chongkai and Dong, Hao},\n  journal={arXiv preprint arXiv:2411.03990},\n  year={2024}\n}\n"},{"id":"wu2023leveraging","title":"Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly","authors":[{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yushi Du","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Dong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":["Robot Manipulation","Robotic Assembly","SE(3) Equivariance"],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"IEEE/CVF International Conference on Computer Vision (ICCV)","url":"https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Leveraging_SE3_Equivariance_for_Learning_3D_Geometric_Shape_Assembly_ICCV_2023_paper.pdf","code":"https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly","website":"https://crtie.github.io/SE-3-part-assembly/","abstract":"Shape assembly aims to reassemble parts (or fragments) into a complete object, which is a common task in our daily life. Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair), geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics. Instead of semantic information, this task focuses on geometric information of parts. As the both geometric and pose space of fractured parts are exceptionally large, shape pose prediction becomes a challenging problem. We tackle multi-part geometrically assembly task, leveraging SE(3) equivariance and invariance, which fits the natural characteristic of the task and narrows the solution space.","description":"We tackle multi-part geometrically assembly task, leveraging SE(3) equivariance and invariance, which fits the natural characteristic of the task and narrows the solution space.","selected":true,"preview":"part_assembly.gif","bibtex":"@inproceedings{wu2023leveraging,\n  title={Leveraging se (3) equivariance for learning 3d geometric shape assembly},\n  author={Wu, Ruihai and Tie, Chenrui and Du, Yushi and Zhao, Yan and Dong, Hao},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={15780--15790},\n  year={2023}\n}\n"}],"title":"Selected Publications","enableOnePageMode":false}]],false,false,false]}]]}]]}]}],null,"$L13"]}]
a:["$","$1","h",{"children":[null,[["$","$L14",null,{"children":"$L15"}],null],["$","$L16",null,{"children":["$","div",null,{"hidden":true,"children":["$","$17",null,{"fallback":null,"children":"$L18"}]}]}]]}]
19:I[4431,[],"OutletBoundary"]
1b:I[5278,[],"AsyncMetadataOutlet"]
13:["$","$L19",null,{"children":["$L1a",["$","$L1b",null,{"promise":"$@1c"}]]}]
15:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1a:null
1d:I[622,[],"IconMark"]
1c:{"metadata":[["$","title","0",{"children":"Chenrui Tie (ÈìÅÂÆ∏Áùø)"}],["$","meta","1",{"name":"description","content":"PhD student at National University of Singapore."}],["$","meta","2",{"name":"author","content":"Chenrui Tie"}],["$","meta","3",{"name":"keywords","content":"Chenrui Tie,PhD,Research,National University of Singapore"}],["$","meta","4",{"name":"creator","content":"Chenrui Tie"}],["$","meta","5",{"name":"publisher","content":"Chenrui Tie"}],["$","meta","6",{"property":"og:title","content":"Chenrui Tie (ÈìÅÂÆ∏Áùø)"}],["$","meta","7",{"property":"og:description","content":"PhD student at National University of Singapore."}],["$","meta","8",{"property":"og:site_name","content":"Chenrui Tie's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Chenrui Tie (ÈìÅÂÆ∏Áùø)"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at National University of Singapore."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}],["$","$L1d","15",{}]],"error":null,"digest":"$undefined"}
18:"$1c:metadata"
