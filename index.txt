1:"$Sreact.fragment"
2:I[7558,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"ThemeProvider"]
3:I[9994,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
b:I[7150,[],""]
:HL["/_next/static/css/3c8b863845cad745.css","style"]
0:{"P":null,"b":"JGEI7gNXIzgJjJ4h24DWW","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/3c8b863845cad745.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Chenrui Tie (ÈìÅÂÆ∏Áùø)","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L6","$L7"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L8"]}]}]]}]]}],{"children":["__PAGE__","$L9",{},null,false]},null,false],"$La",false]],"m":"$undefined","G":["$b",[]],"s":false,"S":true}
c:I[7923,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"default"]
d:I[9178,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
e:I[470,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
f:I[5078,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
10:I[2597,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-c7fe3d0e334c1b65.js","974","static/chunks/app/page-eda579cccc25c928.js"],"default"]
16:I[4431,[],"ViewportBoundary"]
18:I[4431,[],"MetadataBoundary"]
19:"$Sreact.suspense"
6:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
7:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
8:["$","$Lc",null,{"lastUpdated":"November 24, 2025"}]
11:T64e,Non-prehensile (NP) manipulation, in which robots alter object states without forming stable grasps (for example, pushing, poking, or sliding), significantly broadens robotic manipulation capabilities when grasping is infeasible or insufficient. However, enabling a unified framework that generalizes across different tasks, objects, and environments while seamlessly integrating non-prehensile and prehensile (P) actions remains challenging: robots must determine when to invoke NP skills, select the appropriate primitive for each context, and compose P and NP strategies into robust, multi-step plans. We introduce ApaptPNP, a vision-language model (VLM)-empowered task and motion planning framework that systematically selects and combines P and NP skills to accomplish diverse manipulation objectives. Our approach leverages a VLM to interpret visual scene observations and textual task descriptions, generating a high-level plan skeleton that prescribes the sequence and coordination of P and NP actions. A digital-twin based object-centric intermediate layer predicts desired object poses, enabling proactive mental rehearsal of manipulation sequences. Finally, a control module synthesizes low-level robot commands, with continuous execution feedback enabling online task plan refinement and adaptive replanning through the VLM. We evaluate ApaptPNP across representative P&NP hybrid manipulation tasks in both simulation and real-world environments. These results underscore the potential of hybrid P&NP manipulation as a crucial step toward general-purpose, human-level robotic manipulation capabilities.12:T60c,Assembly hinges on reliably forming connections between parts; yet most robotic approaches plan assembly sequences and part poses while treating connectors as an afterthought. Connections represent the critical "last mile" of assembly execution, while task planning may sequence operations and motion plan may position parts, the precise establishment of physical connections ultimately determines assembly success or failure. In this paper, we consider connections as first-class primitives in assembly representation, including connector types, specifications, quantities, and placement locations. Drawing inspiration from how humans learn assembly tasks through step-by-step instruction manuals, we present Manual2Skill++, a vision-language framework that automatically extracts structured connection information from assembly manuals. We encode assembly tasks as hierarchical graphs where nodes represent parts and sub-assemblies, and edges explicitly model connection relationships between components. A large-scale vision-language model parses symbolic diagrams and annotations in manuals to instantiate these graphs, leveraging the rich connection knowledge embedded in human-designed instructions. We curate a dataset containing over 20 assembly tasks with diverse connector types to validate our representation extraction approach, and evaluate the complete task understanding-to-execution pipeline across four complex assembly scenarios in simulation, spanning furniture, toys, and manufacturing components with real-world correspondence.13:T595,Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human.14:T481,Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks. However, extensive demonstrations are required for policy robustness and generalization. To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level SE(3) equivariant diffusion model for generating action sequences in complex robot manipulation tasks. Further, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints. We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner. We evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object. Experiments demonstrate superior data efficiency and manipulation proficiency of our proposed method, as well as its ability to generalize to unseen configurations with only a few demonstrations.9:["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$Ld",null,{"author":{"name":"Chenrui Tie","title":"PhD Student","institution":"National University of Singapore","avatar":"/bio.jpeg"},"social":{"email":"chenrui.tie@u.nus.edu","location":"Singapore","location_url":"https://maps.google.com/?q=National+University+of+Singapore","location_details":["School of Computing,","National University of Singapore"],"google_scholar":"https://scholar.google.com/citations?user=lN8cZMMAAAAJ&","orcid":"","github":"https://github.com/crtie/","linkedin":""},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Embodied AI","Data-Efficient Robot Learning","Generalizable Manipulation Skill","Robotic Assembly"],"keywords":["Robot Manipulation","Robot Manipulation","Non-Prehensile Manipulation","Foundation Models","Task and Motion Planning","Robot Manipulation","Robotic Assembly","Foundation Models","Robot Manipulation","Robotic Assembly","Foundation Models","Robot Manipulation","Imitation Learning","SE(3) Equivariance","Diffusion Policy","Robot Manipulation","Affordance Learning","SE(3) Equivariance","Robot Manipulation","Foundation Models","Robot Manipulation","Differentiable Physics"],"maxKeywords":5}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$Le","about",{"content":"I am a second-year Ph.D. student in the School of Computing, National University of Singapore, supervised by Prof. [Lin Shao](https://linsats.github.io/).\nBefore this, I received my B.S. degree from School of EECS, Peking University in 2024. In my undergraduate study, I am privileged to work with Prof. [Hao Dong](https://zsdonghao.github.io/) and Prof. [He Wang](https://hughw19.github.io/).\n\nMy research focuses on enabling robots to **efficiently** acquire **generalizable** manipulation skills. Specifically, I aim to (1) improve data efficiency by incorporating physics priors such as SE(3) equivariance into learning frameworks, and (2) enhance generalization by leveraging high-level abstractions from vision-language models and hierarchical task representations. It's worth mentioning that before studying computer science, I majored in physics, which naturally inspires me to integrate physical intuitions into robotic learning systems.","title":"About"}],["$","$Lf","news",{"items":[{"date":"2026-01","content":"Our work Manual2Skill++ and AdaptPNP has been accepted by ICRA 2025 üéâ"},{"date":"2025-04","content":"Our work Manual2Skill has been accepted by RSS 2025 üéâ"},{"date":"2025-01","content":"Our work ET-SEED has been accepted by ICLR 2025 üéâ"},{"date":"2024-08","content":"Starting my PhD at National University of Singapore, supervised by Prof. Lin Shao"}],"title":"News"}],["$","$L10","featured_publications",{"publications":[{"id":"zhu2025adaptpnp","title":"AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation","authors":[{"name":"Jinxuan Zhu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Xinyi Cao","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yuran Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingxiang Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zixuan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haonan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junting Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"type":"journal","status":"published","tags":["Robot Manipulation","Non-Prehensile Manipulation","Foundation Models","Task and Motion Planning"],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags","researchArea":"machine-learning","journal":"IEEE International Conference on Robotics & Automation (ICRA)","conference":"","url":"https://arxiv.org/abs/2511.11052","code":"","website":"https://sites.google.com/view/adaptpnp/home","abstract":"$11","description":"A vision-language model-empowered framework that adaptively selects and schedules prehensile and non-prehensile skills for various manipulation tasks.","selected":true,"preview":"np.jpeg","bibtex":"@article{zhu2025adaptpnp,\n  title={AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation},\n  author={Zhu, Jinxuan and Tie, Chenrui and Cao, Xinyi and Wang, Yuran and Guo, Jingxiang and Chen, Zixuan and Chen, Haonan and Chen, Junting and Wu, Ruihai and Shao, Lin},\n  journal={arXiv preprint arXiv:2511.11052},\n  year={2025}\n}\n"},{"id":"tie2025manual2skillpp","title":"Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Shengxiang Sun","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yudi Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yanbo Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhongrui Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhouhan Zhong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jinxuan Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiman Pang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haonan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junting Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"type":"journal","status":"published","tags":["Robot Manipulation","Robotic Assembly","Foundation Models"],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags","researchArea":"machine-learning","journal":"IEEE International Conference on Robotics & Automation (ICRA)","conference":"","url":"https://arxiv.org/abs/2510.16344","code":"","website":"","abstract":"$12","description":"We utilizes a VLM to extract structured, connector-aware hierarchical graph representations from assembly manuals, enabling generalizable robotic assembly.","selected":true,"preview":"m2spp.jpeg","bibtex":"@article{tie2025manual2skillpp,\n  title={Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models},\n  author={Tie, Chenrui and Sun, Shengxiang and Lin, Yudi and Wang, Yanbo and Li, Zhongrui and Zhong, Zhouhan and Zhu, Jinxuan and Pang, Yiman and Chen, Haonan and Chen, Junting and others},\n  journal={arXiv preprint arXiv:2510.16344},\n  year={2025}\n}\n"},{"id":"tie2025manual2skill","title":"Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Shengxiang Sun","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Jinxuan Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiwei Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingxiang Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yue Hu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haonan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junting Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":["Robot Manipulation","Robotic Assembly","Foundation Models"],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"Robotics: Science and Systems (RSS)","url":"https://arxiv.org/abs/2502.10090","code":"https://github.com/owensun2004/Manual2Skill","website":"https://owensun2004.github.io/Furniture-Assembly-Web/","abstract":"$13","description":"We propose a novel framework that enables VLM to understand human-designed manuals and acquire robotic skills for furniture assembly tasks.","selected":true,"preview":"chair.gif","bibtex":"@inproceedings{tie2025manual2skill,\n  title={Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Foundation MÂì¶ÁöÑÁªøËâ≤},\n  author={Tie, Chenrui and Sun, Shengxiang and Zhu, Jinxuan and Liu, Yiwei and Guo, Jingxiang and Hu, Yue and Chen, Haonan and Chen, Junting and Wu, Ruihai and Shao, Lin},\n  booktitle={Robotics: Science and Systems (RSS)},\n  year={2025}\n}\n"},{"id":"tie2024et","title":"ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yue Chen","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Boxuan Dong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zeyi Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chongkai Gao‚Ä†","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Dong‚Ä†","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":["Robot Manipulation","Imitation Learning","SE(3) Equivariance","Diffusion Policy"],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","url":"https://arxiv.org/abs/2411.03990","code":"https://github.com/yuechen0614/ET-SEED","website":"https://et-seed.github.io/","abstract":"$14","description":"We propose a new diffusion policy method to tackle tasks with certain symmetry, achieving better data efficiency and spatial generalization.","selected":true,"preview":"etseed.png","bibtex":"@article{tie2024seed,\n  title={Et-seed: Efficient trajectory-level se (3) equivariant diffusion policy},\n  author={Tie, Chenrui and Chen, Yue and Wu, Ruihai and Dong, Boxuan and Li, Zeyi and Gao, Chongkai and Dong, Hao},\n  journal={arXiv preprint arXiv:2411.03990},\n  year={2024}\n}\n"}],"title":"Selected Publications","enableOnePageMode":false}]],false,false,false]}]]}]]}]}],null,"$L15"]}]
a:["$","$1","h",{"children":[null,[["$","$L16",null,{"children":"$L17"}],null],["$","$L18",null,{"children":["$","div",null,{"hidden":true,"children":["$","$19",null,{"fallback":null,"children":"$L1a"}]}]}]]}]
1b:I[4431,[],"OutletBoundary"]
1d:I[5278,[],"AsyncMetadataOutlet"]
15:["$","$L1b",null,{"children":["$L1c",["$","$L1d",null,{"promise":"$@1e"}]]}]
17:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1c:null
1f:I[622,[],"IconMark"]
1e:{"metadata":[["$","title","0",{"children":"Chenrui Tie (ÈìÅÂÆ∏Áùø)"}],["$","meta","1",{"name":"description","content":"PhD student at National University of Singapore."}],["$","meta","2",{"name":"author","content":"Chenrui Tie"}],["$","meta","3",{"name":"keywords","content":"Chenrui Tie,PhD,Research,National University of Singapore"}],["$","meta","4",{"name":"creator","content":"Chenrui Tie"}],["$","meta","5",{"name":"publisher","content":"Chenrui Tie"}],["$","meta","6",{"property":"og:title","content":"Chenrui Tie (ÈìÅÂÆ∏Áùø)"}],["$","meta","7",{"property":"og:description","content":"PhD student at National University of Singapore."}],["$","meta","8",{"property":"og:site_name","content":"Chenrui Tie's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Chenrui Tie (ÈìÅÂÆ∏Áùø)"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at National University of Singapore."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}],["$","$L1f","15",{}]],"error":null,"digest":"$undefined"}
1a:"$1e:metadata"
