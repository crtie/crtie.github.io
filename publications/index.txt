1:"$Sreact.fragment"
2:I[7558,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"ThemeProvider"]
3:I[9994,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
c:I[7150,[],""]
:HL["/_next/static/css/3c8b863845cad745.css","style"]
0:{"P":null,"b":"mP7IeFZoPsAMh7wVG5Zku","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/3c8b863845cad745.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Chenrui Tie (铁宸睿)","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L6","$L7"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L8"]}]}]]}]]}],{"children":[["slug","publications","d"],"$L9",{"children":["__PAGE__","$La",{},null,false]},null,false]},null,false],"$Lb",false]],"m":"$undefined","G":["$c",[]],"s":false,"S":true}
d:I[7923,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-930ec979fb2e89b3.js","177","static/chunks/app/layout-35d05bc133bf300c.js"],"default"]
f:I[4431,[],"OutletBoundary"]
11:I[5278,[],"AsyncMetadataOutlet"]
13:I[4431,[],"ViewportBoundary"]
15:I[4431,[],"MetadataBoundary"]
16:"$Sreact.suspense"
6:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
7:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
8:["$","$Ld",null,{"lastUpdated":"November 24, 2025"}]
9:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
a:["$","$1","c",{"children":["$Le",null,["$","$Lf",null,{"children":["$L10",["$","$L11",null,{"promise":"$@12"}]]}]]}]
b:["$","$1","h",{"children":[null,[["$","$L13",null,{"children":"$L14"}],null],["$","$L15",null,{"children":["$","div",null,{"hidden":true,"children":["$","$16",null,{"fallback":null,"children":"$L17"}]}]}]]}]
18:I[9958,["247","static/chunks/247-e0831dccaa8864d2.js","987","static/chunks/987-b73e2b43c1b068be.js","681","static/chunks/681-c7fe3d0e334c1b65.js","182","static/chunks/app/%5Bslug%5D/page-3f2c3cab1989051c.js"],"default"]
19:T64e,Non-prehensile (NP) manipulation, in which robots alter object states without forming stable grasps (for example, pushing, poking, or sliding), significantly broadens robotic manipulation capabilities when grasping is infeasible or insufficient. However, enabling a unified framework that generalizes across different tasks, objects, and environments while seamlessly integrating non-prehensile and prehensile (P) actions remains challenging: robots must determine when to invoke NP skills, select the appropriate primitive for each context, and compose P and NP strategies into robust, multi-step plans. We introduce ApaptPNP, a vision-language model (VLM)-empowered task and motion planning framework that systematically selects and combines P and NP skills to accomplish diverse manipulation objectives. Our approach leverages a VLM to interpret visual scene observations and textual task descriptions, generating a high-level plan skeleton that prescribes the sequence and coordination of P and NP actions. A digital-twin based object-centric intermediate layer predicts desired object poses, enabling proactive mental rehearsal of manipulation sequences. Finally, a control module synthesizes low-level robot commands, with continuous execution feedback enabling online task plan refinement and adaptive replanning through the VLM. We evaluate ApaptPNP across representative P&NP hybrid manipulation tasks in both simulation and real-world environments. These results underscore the potential of hybrid P&NP manipulation as a crucial step toward general-purpose, human-level robotic manipulation capabilities.1a:T60c,Assembly hinges on reliably forming connections between parts; yet most robotic approaches plan assembly sequences and part poses while treating connectors as an afterthought. Connections represent the critical "last mile" of assembly execution, while task planning may sequence operations and motion plan may position parts, the precise establishment of physical connections ultimately determines assembly success or failure. In this paper, we consider connections as first-class primitives in assembly representation, including connector types, specifications, quantities, and placement locations. Drawing inspiration from how humans learn assembly tasks through step-by-step instruction manuals, we present Manual2Skill++, a vision-language framework that automatically extracts structured connection information from assembly manuals. We encode assembly tasks as hierarchical graphs where nodes represent parts and sub-assemblies, and edges explicitly model connection relationships between components. A large-scale vision-language model parses symbolic diagrams and annotations in manuals to instantiate these graphs, leveraging the rich connection knowledge embedded in human-designed instructions. We curate a dataset containing over 20 assembly tasks with diverse connector types to validate our representation extraction approach, and evaluate the complete task understanding-to-execution pipeline across four complex assembly scenarios in simulation, spanning furniture, toys, and manufacturing components with real-world correspondence.1b:T595,Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human.1c:T481,Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks. However, extensive demonstrations are required for policy robustness and generalization. To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level SE(3) equivariant diffusion model for generating action sequences in complex robot manipulation tasks. Further, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints. We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner. We evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object. Experiments demonstrate superior data efficiency and manipulation proficiency of our proposed method, as well as its ability to generalize to unseen configurations with only a few demonstrations.1d:T4bb,To substantially enhance robot intelligence, there is a pressing need to develop a large model that enables general-purpose robots to proficiently undertake a broad spectrum of manipulation tasks, akin to the versatile task-planning ability exhibited by LLMs. The vast diversity in objects, robots, and manipulation tasks presents huge challenges. Our work introduces a comprehensive framework to develop a foundation model for general robotic manipulation that formalizes a manipulation task as contact synthesis. Specifically, our model takes as input object and robot manipulator point clouds, object physical attributes, target motions, and manipulation region masks. It outputs contact points on the object and associated contact forces or post-contact motions for robots to achieve the desired manipulation task. We perform extensive experiments both in the simulation and real-world settings, manipulating articulated rigid objects, rigid objects, and deformable objects that vary in dimensionality, ranging from one-dimensional objects like ropes to two-dimensional objects like cloth and extending to three-dimensional objects such as plasticine. Our model achieves average success rates of around 90%.e:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L18",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"zhu2025adaptpnp","title":"AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation","authors":[{"name":"Jinxuan Zhu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Xinyi Cao","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yuran Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingxiang Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zixuan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haonan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junting Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":["Robot Manipulation","Non-Prehensile Manipulation","Foundation Models","Task and Motion Planning"],"keywords":"$e:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"Preprint","conference":"","url":"https://arxiv.org/abs/2511.11052","code":"","website":"https://sites.google.com/view/adaptpnp/home","abstract":"$19","description":"A vision-language model-empowered framework that adaptively selects and schedules prehensile and non-prehensile skills for various manipulation tasks.","selected":false,"preview":"np.jpeg","bibtex":"@article{zhu2025adaptpnp,\n  title={AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation},\n  author={Zhu, Jinxuan and Tie, Chenrui and Cao, Xinyi and Wang, Yuran and Guo, Jingxiang and Chen, Zixuan and Chen, Haonan and Chen, Junting and Wu, Ruihai and Shao, Lin},\n  journal={arXiv preprint arXiv:2511.11052},\n  year={2025}\n}\n"},{"id":"tie2025manual2skillpp","title":"Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Shengxiang Sun","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yudi Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yanbo Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhongrui Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhouhan Zhong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jinxuan Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiman Pang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haonan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junting Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":["Robot Manipulation","Robotic Assembly","Foundation Models"],"keywords":"$e:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"Preprint","conference":"","url":"https://arxiv.org/abs/2510.16344","code":"","website":"","abstract":"$1a","description":"We utilizes a VLM to extract structured, connector-aware hierarchical graph representations from assembly manuals, enabling generalizable robotic assembly.","selected":false,"preview":"m2spp.jpeg","bibtex":"@article{tie2025manual2skillpp,\n  title={Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models},\n  author={Tie, Chenrui and Sun, Shengxiang and Lin, Yudi and Wang, Yanbo and Li, Zhongrui and Zhong, Zhouhan and Zhu, Jinxuan and Pang, Yiman and Chen, Haonan and Chen, Junting and others},\n  journal={arXiv preprint arXiv:2510.16344},\n  year={2025}\n}\n"},{"id":"tie2025manual2skill","title":"Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Shengxiang Sun","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Jinxuan Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiwei Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingxiang Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yue Hu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haonan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junting Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":["Robot Manipulation","Robotic Assembly","Foundation Models"],"keywords":"$e:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"Robotics: Science and Systems (RSS)","url":"https://arxiv.org/abs/2502.10090","code":"https://github.com/owensun2004/Manual2Skill","website":"https://owensun2004.github.io/Furniture-Assembly-Web/","abstract":"$1b","description":"We propose a novel framework that enables VLM to understand human-designed manuals and acquire robotic skills for furniture assembly tasks.","selected":true,"preview":"chair.gif","bibtex":"@inproceedings{tie2025manual2skill,\n  title={Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Foundation M哦的绿色},\n  author={Tie, Chenrui and Sun, Shengxiang and Zhu, Jinxuan and Liu, Yiwei and Guo, Jingxiang and Hu, Yue and Chen, Haonan and Chen, Junting and Wu, Ruihai and Shao, Lin},\n  booktitle={Robotics: Science and Systems (RSS)},\n  year={2025}\n}\n"},{"id":"tie2024et","title":"ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy","authors":[{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yue Chen","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Boxuan Dong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zeyi Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chongkai Gao†","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Dong†","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":["Robot Manipulation","Imitation Learning","SE(3) Equivariance","Diffusion Policy"],"keywords":"$e:props:children:0:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","url":"https://arxiv.org/abs/2411.03990","code":"https://github.com/yuechen0614/ET-SEED","website":"https://et-seed.github.io/","abstract":"$1c","description":"We propose a new diffusion policy method to tackle tasks with certain symmetry, achieving better data efficiency and spatial generalization.","selected":true,"preview":"etseed.png","bibtex":"@article{tie2024seed,\n  title={Et-seed: Efficient trajectory-level se (3) equivariant diffusion policy},\n  author={Tie, Chenrui and Chen, Yue and Wu, Ruihai and Dong, Boxuan and Li, Zeyi and Gao, Chongkai and Dong, Hao},\n  journal={arXiv preprint arXiv:2411.03990},\n  year={2024}\n}\n"},{"id":"chen2024eqvafford","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","authors":[{"name":"Yue Chen","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Hao Dong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":["Robot Manipulation","Affordance Learning","SE(3) Equivariance"],"keywords":"$e:props:children:0:props:publications:4:tags","researchArea":"machine-learning","journal":"","conference":"CVPR 2024 Workshop EquiVision","url":"https://arxiv.org/abs/2408.01953","code":"","website":"","abstract":"Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.","description":"We propose EqvAfford framework, with novel designs to guarantee the SE(3) equivariance in point-level affordance learning for downstream robotic manipulation.","selected":false,"preview":"eqvafford.png","bibtex":"@article{chen2024eqvafford,\n  title={EqvAfford: SE (3) equivariance for point-level affordance learning},\n  author={Chen, Yue and Tie, Chenrui and Wu, Ruihai and Dong, Hao},\n  journal={arXiv preprint arXiv:2408.01953},\n  year={2024}\n}\n"},{"id":"xu2024manifoundation","title":"ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots","authors":[{"name":"Zhixuan Xu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Chongkai Gao","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Zixuan Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Gang Yang","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haozhuo Zheng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haoyu Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Weikun Peng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Debang Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tianyi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhouliang Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":["Robot Manipulation","Foundation Models"],"keywords":"$e:props:children:0:props:publications:5:tags","researchArea":"machine-learning","journal":"","conference":"IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","url":"https://arxiv.org/abs/2405.06964","code":"https://github.com/NUS-LinS-Lab/ManiFM","website":"https://manifoundationmodel.github.io/","abstract":"$1d","description":"We Introduce a framework taking contact synthesis as a unified task representation that can generalizes over objects, robots, and manipulation tasks.","selected":false,"preview":"manifm_.png","bibtex":"@inproceedings{xu2024manifoundation,\n  title={Manifoundation model for general-purpose robotic manipulation of contact synthesis with arbitrary objects and robots},\n  author={Xu, Zhixuan and Gao, Chongkai and Liu, Zixuan and Yang, Gang and Tie, Chenrui and Zheng, Haozhuo and Zhou, Haoyu and Peng, Weikun and Wang, Debang and Chen, Tianyi and others},\n  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n  pages={2573--2580},\n  year={2024},\n  organization={IEEE}\n}\n"},{"id":"yang2024jade","title":"Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact","authors":[{"name":"Gang Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Siyuan Luo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yunhai Feng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhixin Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":["Robot Manipulation","Differentiable Physics"],"keywords":"$e:props:children:0:props:publications:6:tags","researchArea":"machine-learning","journal":"","conference":"IEEE International Conference on Robotics and Automation (ICRA)","url":"https://ieeexplore.ieee.org/document/10610750","code":"https://github.com/NUS-LinS-Lab/Jade","website":"https://sites.google.com/view/diffsim/","abstract":"We present Jade, a differentiable physics engine for articulated rigid bodies. Jade models contacts as the Linear Complementarity Problem (LCP). Compared to existing differentiable simulations, Jade offers features including intersection-free collision simulation and stable LCP solutions for multiple frictional contacts. We use continuous collision detection to detect the time of impact and adopt the backtracking strategy to prevent intersection between bodies with complex geometry shapes. We derive the gradient calculation to ensure the whole simulation process is differentiable under the backtracking mechanism. We modify the popular Dantzig's algorithm to get valid solutions under multiple frictional contacts. We conduct extensive experiments to demonstrate the effectiveness of our differentiable physics simulation over a variety of contact-rich tasks.","description":"We developed Jade, a differentiable physics engine for articulated rigid bodies, using the continuous collision detection to prevent intersection between bodies with complex geometry shapes.","selected":false,"preview":"jade.png","bibtex":"@inproceedings{yang2024jade,\n  title={Jade: A differentiable physics engine for articulated rigid bodies with intersection-free frictional contact},\n  author={Yang, Gang and Luo, Siyuan and Feng, Yunhai and Sun, Zhixin and Tie, Chenrui and Shao, Lin},\n  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={9488--9494},\n  year={2024},\n  organization={IEEE}\n}\n"},{"id":"wu2023leveraging","title":"Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly","authors":[{"name":"Ruihai Wu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Chenrui Tie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Yushi Du","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Dong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":["Robot Manipulation","Robotic Assembly","SE(3) Equivariance"],"keywords":"$e:props:children:0:props:publications:7:tags","researchArea":"machine-learning","journal":"","conference":"IEEE/CVF International Conference on Computer Vision (ICCV)","url":"https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Leveraging_SE3_Equivariance_for_Learning_3D_Geometric_Shape_Assembly_ICCV_2023_paper.pdf","code":"https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly","website":"https://crtie.github.io/SE-3-part-assembly/","abstract":"Shape assembly aims to reassemble parts (or fragments) into a complete object, which is a common task in our daily life. Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair), geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics. Instead of semantic information, this task focuses on geometric information of parts. As the both geometric and pose space of fractured parts are exceptionally large, shape pose prediction becomes a challenging problem. We tackle multi-part geometrically assembly task, leveraging SE(3) equivariance and invariance, which fits the natural characteristic of the task and narrows the solution space.","description":"We tackle multi-part geometrically assembly task, leveraging SE(3) equivariance and invariance, which fits the natural characteristic of the task and narrows the solution space.","selected":true,"preview":"part_assembly.gif","bibtex":"@inproceedings{wu2023leveraging,\n  title={Leveraging se (3) equivariance for learning 3d geometric shape assembly},\n  author={Wu, Ruihai and Tie, Chenrui and Du, Yushi and Zhao, Yan and Dong, Hao},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={15780--15790},\n  year={2023}\n}\n"}]}],false,false]}]
14:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
10:null
1e:I[622,[],"IconMark"]
12:{"metadata":[["$","title","0",{"children":"Publications | Chenrui Tie (铁宸睿)"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Chenrui Tie"}],["$","meta","3",{"name":"keywords","content":"Chenrui Tie,PhD,Research,National University of Singapore"}],["$","meta","4",{"name":"creator","content":"Chenrui Tie"}],["$","meta","5",{"name":"publisher","content":"Chenrui Tie"}],["$","meta","6",{"property":"og:title","content":"Chenrui Tie (铁宸睿)"}],["$","meta","7",{"property":"og:description","content":"PhD student at National University of Singapore."}],["$","meta","8",{"property":"og:site_name","content":"Chenrui Tie's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Chenrui Tie (铁宸睿)"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at National University of Singapore."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}],["$","$L1e","15",{}]],"error":null,"digest":"$undefined"}
17:"$12:metadata"
