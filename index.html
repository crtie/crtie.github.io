<!DOCTYPE html><!--I9scN3QJdapttO64XzavQ--><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpeg"/><link rel="stylesheet" href="/_next/static/css/16cccdf86f8b2c19.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-32b99883bf220ffe.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-687568fb72c1c1e1.js" async=""></script><script src="/_next/static/chunks/main-app-27c1e69e79f58887.js" async=""></script><script src="/_next/static/chunks/247-e0831dccaa8864d2.js" async=""></script><script src="/_next/static/chunks/619-3ba632d834111881.js" async=""></script><script src="/_next/static/chunks/918-930ec979fb2e89b3.js" async=""></script><script src="/_next/static/chunks/app/layout-35d05bc133bf300c.js" async=""></script><script src="/_next/static/chunks/987-b73e2b43c1b068be.js" async=""></script><script src="/_next/static/chunks/681-38b1a8af1a2d7628.js" async=""></script><script src="/_next/static/chunks/app/page-4c8e3d4545409f49.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><title>Chenrui Tie (ÈìÅÂÆ∏Áùø)</title><meta name="description" content="PhD student at National University of Singapore."/><meta name="author" content="Chenrui Tie"/><meta name="keywords" content="Chenrui Tie,PhD,Research,National University of Singapore"/><meta name="creator" content="Chenrui Tie"/><meta name="publisher" content="Chenrui Tie"/><meta property="og:title" content="Chenrui Tie (ÈìÅÂÆ∏Áùø)"/><meta property="og:description" content="PhD student at National University of Singapore."/><meta property="og:site_name" content="Chenrui Tie&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Chenrui Tie (ÈìÅÂÆ∏Áùø)"/><meta name="twitter:description" content="PhD student at National University of Singapore."/><link rel="icon" href="/favicon.svg"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div hidden=""><!--$--><!--/$--></div><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Chenrui Tie (ÈìÅÂÆ∏Áùø)</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-_R_5pdb_" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Chenrui Tie" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpeg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Chenrui Tie</h1><p class="text-lg text-accent font-medium mb-1">PhD Student</p><p class="text-neutral-600 mb-2">National University of Singapore</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=lN8cZMMAAAAJ&amp;" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://github.com/crtie/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="flex flex-wrap gap-2 justify-center items-center min-h-[100px]"><span class="text-accent font-semibold transition-all duration-200 hover:scale-110 cursor-default whitespace-nowrap" title="Appears in 10 publications" style="font-size:1.5rem;line-height:1.8;opacity:0;transform:scale(0.5)">Robot Manipulation</span><span class="text-neutral-600 dark:text-neutral-500 transition-all duration-200 hover:scale-110 cursor-default whitespace-nowrap" title="Appears in 4 publications" style="font-size:1rem;line-height:1.8;opacity:0;transform:scale(0.5)">Foundation Models</span><span class="text-neutral-600 dark:text-neutral-500 transition-all duration-200 hover:scale-110 cursor-default whitespace-nowrap" title="Appears in 3 publications" style="font-size:0.9166666666666666rem;line-height:1.8;opacity:0;transform:scale(0.5)">Robotic Assembly</span><span class="text-neutral-600 dark:text-neutral-500 transition-all duration-200 hover:scale-110 cursor-default whitespace-nowrap" title="Appears in 3 publications" style="font-size:0.9166666666666666rem;line-height:1.8;opacity:0;transform:scale(0.5)">SE(3) Equivariance</span><span class="text-neutral-600 dark:text-neutral-500 transition-all duration-200 hover:scale-110 cursor-default whitespace-nowrap" title="Appears in 1 publication" style="font-size:0.75rem;line-height:1.8;opacity:0;transform:scale(0.5)"></span></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a second-year Ph.D. student in the School of Computing, National University of Singapore, supervised by Prof. <a href="https://linsats.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Lin Shao</a>.
Before this, I received my B.S. degree from School of EECS, Peking University in 2024. In my undergraduate study, I am privileged to work with Prof. <a href="https://zsdonghao.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Hao Dong</a> and Prof. <a href="https://hughw19.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">He Wang</a>.</p>
<p class="mb-4 last:mb-0">My research focuses on enabling robots to <strong class="font-semibold text-primary">efficiently</strong> acquire <strong class="font-semibold text-primary">generalizable</strong> manipulation skills. Specifically, I aim to (1) improve data efficiency by incorporating physics priors such as SE(3) equivariance into learning frameworks, and (2) enhance generalization by leveraging high-level abstractions from vision-language models and hierarchical task representations. It&#x27;s worth mentioning that before studying computer science, I majored in physics, which naturally inspires me to integrate physical intuitions into robotic learning systems.</p></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-01</span><p class="text-sm text-neutral-700">Our work Manual2Skill++ and AdaptPNP has been accepted by ICRA 2025 üéâ</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-04</span><p class="text-sm text-neutral-700">Our work Manual2Skill has been accepted by RSS 2025 üéâ</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-01</span><p class="text-sm text-neutral-700">Our work ET-SEED has been accepted by ICLR 2025 üéâ</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-08</span><p class="text-sm text-neutral-700">Starting my PhD at National University of Singapore, supervised by Prof. Lin Shao</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All ‚Üí</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Jinxuan Zhu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="font-bold text-neutral-900 dark:text-neutral-200">Chenrui Tie</span><sup class="ml-0 text-neutral-900 dark:text-neutral-200">‚Ä†</sup>, </span><span><span class="">Xinyi Cao</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Yuran Wang</span>, </span><span><span class="">Jingxiang Guo</span>, </span><span><span class="">Zixuan Chen</span>, </span><span><span class="">Haonan Chen</span>, </span><span><span class="">Junting Chen</span>, </span><span><span class="">Ruihai Wu</span>, </span><span><span class="">Lin Shao</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE International Conference on Robotics &amp; Automation (ICRA)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A vision-language model-empowered framework that adaptively selects and schedules prehensile and non-prehensile skills for various manipulation tasks.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-bold text-neutral-900 dark:text-neutral-200">Chenrui Tie</span><sup class="ml-0 text-neutral-900 dark:text-neutral-200">‚Ä†</sup>, </span><span><span class="">Shengxiang Sun</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Yudi Lin</span>, </span><span><span class="">Yanbo Wang</span>, </span><span><span class="">Zhongrui Li</span>, </span><span><span class="">Zhouhan Zhong</span>, </span><span><span class="">Jinxuan Zhu</span>, </span><span><span class="">Yiman Pang</span>, </span><span><span class="">Haonan Chen</span>, </span><span><span class="">Junting Chen</span>, </span><span><span class="">Ruihai Wu</span>, </span><span><span class="">Lin Shao</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE International Conference on Robotics &amp; Automation (ICRA)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">We utilizes a VLM to extract structured, connector-aware hierarchical graph representations from assembly manuals, enabling generalizable robotic assembly.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-bold text-neutral-900 dark:text-neutral-200">Chenrui Tie</span><sup class="ml-0 text-neutral-900 dark:text-neutral-200">‚Ä†</sup>, </span><span><span class="">Shengxiang Sun</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Jinxuan Zhu</span>, </span><span><span class="">Yiwei Liu</span>, </span><span><span class="">Jingxiang Guo</span>, </span><span><span class="">Yue Hu</span>, </span><span><span class="">Haonan Chen</span>, </span><span><span class="">Junting Chen</span>, </span><span><span class="">Ruihai Wu</span>, </span><span><span class="">Lin Shao</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Robotics: Science and Systems (RSS)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">We propose a novel framework that enables VLM to understand human-designed manuals and acquire robotic skills for furniture assembly tasks.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-bold text-neutral-900 dark:text-neutral-200">Chenrui Tie</span><sup class="ml-0 text-neutral-900 dark:text-neutral-200">‚Ä†</sup>, </span><span><span class="">Yue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Ruihai Wu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Boxuan Dong</span>, </span><span><span class="">Zeyi Li</span>, </span><span><span class="">Chongkai Gao‚Ä†</span>, </span><span><span class="">Hao Dong‚Ä†</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">International Conference on Learning Representations (ICLR)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">We propose a new diffusion policy method to tackle tasks with certain symmetry, achieving better data efficiency and spatial generalization.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Ruihai Wu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="font-bold text-neutral-900 dark:text-neutral-200">Chenrui Tie</span><sup class="ml-0 text-neutral-900 dark:text-neutral-200">‚Ä†</sup>, </span><span><span class="">Yushi Du</span>, </span><span><span class="">Yan Zhao</span>, </span><span><span class="">Hao Dong</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE/CVF International Conference on Computer Vision (ICCV)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">We tackle multi-part geometrically assembly task, leveraging SE(3) equivariance and invariance, which fits the natural characteristic of the task and narrows the solution space.</p></div></div></section></section></div></div></div><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->Feb 09, 2026</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-32b99883bf220ffe.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7558,[\"247\",\"static/chunks/247-e0831dccaa8864d2.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"918\",\"static/chunks/918-930ec979fb2e89b3.js\",\"177\",\"static/chunks/app/layout-35d05bc133bf300c.js\"],\"ThemeProvider\"]\n3:I[9994,[\"247\",\"static/chunks/247-e0831dccaa8864d2.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"918\",\"static/chunks/918-930ec979fb2e89b3.js\",\"177\",\"static/chunks/app/layout-35d05bc133bf300c.js\"],\"default\"]\n4:I[9766,[],\"\"]\n5:I[8924,[],\"\"]\nb:I[7150,[],\"\"]\n:HL[\"/_next/static/css/16cccdf86f8b2c19.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"I9scN3QJdapttO64XzavQ\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/16cccdf86f8b2c19.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Chenrui Tie (ÈìÅÂÆ∏Áùø)\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],\"$L6\",\"$L7\"]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],\"$L8\"]}]}]]}]]}],{\"children\":[\"__PAGE__\",\"$L9\",{},null,false]},null,false],\"$La\",false]],\"m\":\"$undefined\",\"G\":[\"$b\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:I[7923,[\"247\",\"static/chunks/247-e0831dccaa8864d2.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"918\",\"static/chunks/918-930ec979fb2e89b3.js\",\"177\",\"static/chunks/app/layout-35d05bc133bf300c.js\"],\"default\"]\nd:I[9178,[\"247\",\"static/chunks/247-e0831dccaa8864d2.js\",\"987\",\"static/chunks/987-b73e2b43c1b068be.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"681\",\"static/chunks/681-38b1a8af1a2d7628.js\",\"974\",\"static/chunks/app/page-4c8e3d4545409f49.js\"],\"default\"]\ne:I[470,[\"247\",\"static/chunks/247-e0831dccaa8864d2.js\",\"987\",\"static/chunks/987-b73e2b43c1b068be.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"681\",\"static/chunks/681-38b1a8af1a2d7628.js\",\"974\",\"static/chunks/app/page-4c8e3d4545409f49.js\"],\"default\"]\nf:I[5078,[\"247\",\"static/chunks/247-e0831dccaa8864d2.js\",\"987\",\"static/chunks/987-b73e2b43c1b068be.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"681\",\"static/chunks/681-38b1a8af1a2d7628.js\",\"974\",\"static/chunks/app/page-4c8e3d4545409f49.js\"],\"default\"]\n10:I[2597,[\"247\",\"static/chunks/247-e0831dccaa8864d2.js\",\"987\",\"static/chunks/987-b73e2b43c1b068be.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"681\",\"static/chunks/681-38b1a8af1a2d7628.js\",\"974\",\"static/chunks/app/page-4c8e3d4545409f49.js\"],\"default\"]\n16:I[4431,[],\"ViewportBoundary\"]\n18:I[4431,[],\"MetadataBoundary\"]\n19:\"$Sreact.suspense\"\n6:[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}]\n7:[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]\n8:[\"$\",\"$Lc\",null,{\"lastUpdated\":\"Feb 09, 2026\"}]\n11:T64e,"])</script><script>self.__next_f.push([1,"Non-prehensile (NP) manipulation, in which robots alter object states without forming stable grasps (for example, pushing, poking, or sliding), significantly broadens robotic manipulation capabilities when grasping is infeasible or insufficient. However, enabling a unified framework that generalizes across different tasks, objects, and environments while seamlessly integrating non-prehensile and prehensile (P) actions remains challenging: robots must determine when to invoke NP skills, select the appropriate primitive for each context, and compose P and NP strategies into robust, multi-step plans. We introduce ApaptPNP, a vision-language model (VLM)-empowered task and motion planning framework that systematically selects and combines P and NP skills to accomplish diverse manipulation objectives. Our approach leverages a VLM to interpret visual scene observations and textual task descriptions, generating a high-level plan skeleton that prescribes the sequence and coordination of P and NP actions. A digital-twin based object-centric intermediate layer predicts desired object poses, enabling proactive mental rehearsal of manipulation sequences. Finally, a control module synthesizes low-level robot commands, with continuous execution feedback enabling online task plan refinement and adaptive replanning through the VLM. We evaluate ApaptPNP across representative P\u0026NP hybrid manipulation tasks in both simulation and real-world environments. These results underscore the potential of hybrid P\u0026NP manipulation as a crucial step toward general-purpose, human-level robotic manipulation capabilities."])</script><script>self.__next_f.push([1,"12:T60c,"])</script><script>self.__next_f.push([1,"Assembly hinges on reliably forming connections between parts; yet most robotic approaches plan assembly sequences and part poses while treating connectors as an afterthought. Connections represent the critical \"last mile\" of assembly execution, while task planning may sequence operations and motion plan may position parts, the precise establishment of physical connections ultimately determines assembly success or failure. In this paper, we consider connections as first-class primitives in assembly representation, including connector types, specifications, quantities, and placement locations. Drawing inspiration from how humans learn assembly tasks through step-by-step instruction manuals, we present Manual2Skill++, a vision-language framework that automatically extracts structured connection information from assembly manuals. We encode assembly tasks as hierarchical graphs where nodes represent parts and sub-assemblies, and edges explicitly model connection relationships between components. A large-scale vision-language model parses symbolic diagrams and annotations in manuals to instantiate these graphs, leveraging the rich connection knowledge embedded in human-designed instructions. We curate a dataset containing over 20 assembly tasks with diverse connector types to validate our representation extraction approach, and evaluate the complete task understanding-to-execution pipeline across four complex assembly scenarios in simulation, spanning furniture, toys, and manufacturing components with real-world correspondence."])</script><script>self.__next_f.push([1,"13:T595,"])</script><script>self.__next_f.push([1,"Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human."])</script><script>self.__next_f.push([1,"14:T481,"])</script><script>self.__next_f.push([1,"Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks. However, extensive demonstrations are required for policy robustness and generalization. To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level SE(3) equivariant diffusion model for generating action sequences in complex robot manipulation tasks. Further, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints. We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner. We evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object. Experiments demonstrate superior data efficiency and manipulation proficiency of our proposed method, as well as its ability to generalize to unseen configurations with only a few demonstrations."])</script><script>self.__next_f.push([1,"9:[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$Ld\",null,{\"author\":{\"name\":\"Chenrui Tie\",\"title\":\"PhD Student\",\"institution\":\"National University of Singapore\",\"avatar\":\"/bio.jpeg\"},\"social\":{\"email\":\"chenrui.tie@u.nus.edu\",\"location\":\"Singapore\",\"location_url\":\"https://maps.google.com/?q=National+University+of+Singapore\",\"location_details\":[\"School of Computing,\",\"National University of Singapore\"],\"google_scholar\":\"https://scholar.google.com/citations?user=lN8cZMMAAAAJ\u0026\",\"orcid\":\"\",\"github\":\"https://github.com/crtie/\",\"linkedin\":\"\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"Embodied AI\",\"Data-Efficient Robot Learning\",\"Generalizable Manipulation Skill\",\"Robotic Assembly\"],\"keywords\":[\"\",\"Robot Manipulation\",\"Foundation Models.\",\"Robot Manipulation\",\"Robot Manipulation\",\"Non-Prehensile Manipulation\",\"Foundation Models\",\"Task and Motion Planning\",\"Robot Manipulation\",\"Robotic Assembly\",\"Foundation Models\",\"Robot Manipulation\",\"Robotic Assembly\",\"Foundation Models\",\"Robot Manipulation\",\"Imitation Learning\",\"SE(3) Equivariance\",\"Diffusion Policy\",\"Robot Manipulation\",\"Affordance Learning\",\"SE(3) Equivariance\",\"Robot Manipulation\",\"Foundation Models\",\"Robot Manipulation\",\"Differentiable Physics\",\"Robot Manipulation\",\"Robotic Assembly\",\"SE(3) Equivariance\"],\"maxKeywords\":5}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$Le\",\"about\",{\"content\":\"I am a second-year Ph.D. student in the School of Computing, National University of Singapore, supervised by Prof. [Lin Shao](https://linsats.github.io/).\\nBefore this, I received my B.S. degree from School of EECS, Peking University in 2024. In my undergraduate study, I am privileged to work with Prof. [Hao Dong](https://zsdonghao.github.io/) and Prof. [He Wang](https://hughw19.github.io/).\\n\\nMy research focuses on enabling robots to **efficiently** acquire **generalizable** manipulation skills. Specifically, I aim to (1) improve data efficiency by incorporating physics priors such as SE(3) equivariance into learning frameworks, and (2) enhance generalization by leveraging high-level abstractions from vision-language models and hierarchical task representations. It's worth mentioning that before studying computer science, I majored in physics, which naturally inspires me to integrate physical intuitions into robotic learning systems.\",\"title\":\"About\"}],[\"$\",\"$Lf\",\"news\",{\"items\":[{\"date\":\"2026-01\",\"content\":\"Our work Manual2Skill++ and AdaptPNP has been accepted by ICRA 2025 üéâ\"},{\"date\":\"2025-04\",\"content\":\"Our work Manual2Skill has been accepted by RSS 2025 üéâ\"},{\"date\":\"2025-01\",\"content\":\"Our work ET-SEED has been accepted by ICLR 2025 üéâ\"},{\"date\":\"2024-08\",\"content\":\"Starting my PhD at National University of Singapore, supervised by Prof. Lin Shao\"}],\"title\":\"News\"}],[\"$\",\"$L10\",\"featured_publications\",{\"publications\":[{\"id\":\"zhu2025adaptpnp\",\"title\":\"AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation\",\"authors\":[{\"name\":\"Jinxuan Zhu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Chenrui Tie\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xinyi Cao\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Yuran Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingxiang Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zixuan Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haonan Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junting Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruihai Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lin Shao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Robot Manipulation\",\"Non-Prehensile Manipulation\",\"Foundation Models\",\"Task and Motion Planning\"],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"IEEE International Conference on Robotics \u0026 Automation (ICRA)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2511.11052\",\"code\":\"\",\"website\":\"https://adaptpnp.github.io/\",\"abstract\":\"$11\",\"description\":\"A vision-language model-empowered framework that adaptively selects and schedules prehensile and non-prehensile skills for various manipulation tasks.\",\"selected\":true,\"preview\":\"np.jpeg\",\"bibtex\":\"@article{zhu2025adaptpnp,\\n  title={AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation},\\n  author={Zhu, Jinxuan and Tie, Chenrui and Cao, Xinyi and Wang, Yuran and Guo, Jingxiang and Chen, Zixuan and Chen, Haonan and Chen, Junting and Wu, Ruihai and Shao, Lin},\\n  journal={arXiv preprint arXiv:2511.11052},\\n  year={2025}\\n}\\n\"},{\"id\":\"tie2025manual2skillpp\",\"title\":\"Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models\",\"authors\":[{\"name\":\"Chenrui Tie\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Shengxiang Sun\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Yudi Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanbo Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhongrui Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhouhan Zhong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jinxuan Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiman Pang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haonan Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junting Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruihai Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lin Shao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Robot Manipulation\",\"Robotic Assembly\",\"Foundation Models\"],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"IEEE International Conference on Robotics \u0026 Automation (ICRA)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2510.16344\",\"code\":\"\",\"website\":\"https://nus-lins-lab.github.io/Manual2SkillPP/\",\"abstract\":\"$12\",\"description\":\"We utilizes a VLM to extract structured, connector-aware hierarchical graph representations from assembly manuals, enabling generalizable robotic assembly.\",\"selected\":true,\"preview\":\"m2spp.jpeg\",\"bibtex\":\"@article{tie2025manual2skillpp,\\n  title={Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models},\\n  author={Tie, Chenrui and Sun, Shengxiang and Lin, Yudi and Wang, Yanbo and Li, Zhongrui and Zhong, Zhouhan and Zhu, Jinxuan and Pang, Yiman and Chen, Haonan and Chen, Junting and others},\\n  journal={arXiv preprint arXiv:2510.16344},\\n  year={2025}\\n}\\n\"},{\"id\":\"tie2025manual2skill\",\"title\":\"Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models\",\"authors\":[{\"name\":\"Chenrui Tie\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Shengxiang Sun\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Jinxuan Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiwei Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingxiang Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yue Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haonan Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junting Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruihai Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lin Shao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Robot Manipulation\",\"Robotic Assembly\",\"Foundation Models\"],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Robotics: Science and Systems (RSS)\",\"url\":\"https://arxiv.org/abs/2502.10090\",\"code\":\"https://github.com/owensun2004/Manual2Skill\",\"website\":\"https://owensun2004.github.io/Furniture-Assembly-Web/\",\"abstract\":\"$13\",\"description\":\"We propose a novel framework that enables VLM to understand human-designed manuals and acquire robotic skills for furniture assembly tasks.\",\"selected\":true,\"preview\":\"chair.gif\",\"bibtex\":\"@inproceedings{tie2025manual2skill,\\n  title={Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Foundation MÂì¶ÁöÑÁªøËâ≤},\\n  author={Tie, Chenrui and Sun, Shengxiang and Zhu, Jinxuan and Liu, Yiwei and Guo, Jingxiang and Hu, Yue and Chen, Haonan and Chen, Junting and Wu, Ruihai and Shao, Lin},\\n  booktitle={Robotics: Science and Systems (RSS)},\\n  year={2025}\\n}\\n\"},{\"id\":\"tie2024et\",\"title\":\"ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy\",\"authors\":[{\"name\":\"Chenrui Tie\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Yue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Ruihai Wu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Boxuan Dong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zeyi Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chongkai Gao‚Ä†\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Dong‚Ä†\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Robot Manipulation\",\"Imitation Learning\",\"SE(3) Equivariance\",\"Diffusion Policy\"],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations (ICLR)\",\"url\":\"https://arxiv.org/abs/2411.03990\",\"code\":\"https://github.com/yuechen0614/ET-SEED\",\"website\":\"https://et-seed.github.io/\",\"abstract\":\"$14\",\"description\":\"We propose a new diffusion policy method to tackle tasks with certain symmetry, achieving better data efficiency and spatial generalization.\",\"selected\":true,\"preview\":\"etseed.png\",\"bibtex\":\"@article{tie2024seed,\\n  title={Et-seed: Efficient trajectory-level se (3) equivariant diffusion policy},\\n  author={Tie, Chenrui and Chen, Yue and Wu, Ruihai and Dong, Boxuan and Li, Zeyi and Gao, Chongkai and Dong, Hao},\\n  journal={arXiv preprint arXiv:2411.03990},\\n  year={2024}\\n}\\n\"},{\"id\":\"wu2023leveraging\",\"title\":\"Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly\",\"authors\":[{\"name\":\"Ruihai Wu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Chenrui Tie\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Yushi Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yan Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Dong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Robot Manipulation\",\"Robotic Assembly\",\"SE(3) Equivariance\"],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE/CVF International Conference on Computer Vision (ICCV)\",\"url\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Leveraging_SE3_Equivariance_for_Learning_3D_Geometric_Shape_Assembly_ICCV_2023_paper.pdf\",\"code\":\"https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly\",\"website\":\"https://crtie.github.io/SE-3-part-assembly/\",\"abstract\":\"Shape assembly aims to reassemble parts (or fragments) into a complete object, which is a common task in our daily life. Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair), geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics. Instead of semantic information, this task focuses on geometric information of parts. As the both geometric and pose space of fractured parts are exceptionally large, shape pose prediction becomes a challenging problem. We tackle multi-part geometrically assembly task, leveraging SE(3) equivariance and invariance, which fits the natural characteristic of the task and narrows the solution space.\",\"description\":\"We tackle multi-part geometrically assembly task, leveraging SE(3) equivariance and invariance, which fits the natural characteristic of the task and narrows the solution space.\",\"selected\":true,\"preview\":\"part_assembly.gif\",\"bibtex\":\"@inproceedings{wu2023leveraging,\\n  title={Leveraging se (3) equivariance for learning 3d geometric shape assembly},\\n  author={Wu, Ruihai and Tie, Chenrui and Du, Yushi and Zhao, Yan and Dong, Hao},\\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\\n  pages={15780--15790},\\n  year={2023}\\n}\\n\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}]],false,false,false]}]]}]]}]}],null,\"$L15\"]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L16\",null,{\"children\":\"$L17\"}],null],[\"$\",\"$L18\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$19\",null,{\"fallback\":null,\"children\":\"$L1a\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"1b:I[4431,[],\"OutletBoundary\"]\n1d:I[5278,[],\"AsyncMetadataOutlet\"]\n15:[\"$\",\"$L1b\",null,{\"children\":[\"$L1c\",[\"$\",\"$L1d\",null,{\"promise\":\"$@1e\"}]]}]\n"])</script><script>self.__next_f.push([1,"17:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1c:null\n"])</script><script>self.__next_f.push([1,"1f:I[622,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"1e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Chenrui Tie (ÈìÅÂÆ∏Áùø)\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PhD student at National University of Singapore.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Chenrui Tie\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Chenrui Tie,PhD,Research,National University of Singapore\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Chenrui Tie\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Chenrui Tie\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Chenrui Tie (ÈìÅÂÆ∏Áùø)\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at National University of Singapore.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Chenrui Tie's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Chenrui Tie (ÈìÅÂÆ∏Áùø)\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at National University of Singapore.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}],[\"$\",\"$L1f\",\"15\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"1a:\"$1e:metadata\"\n"])</script></body></html>